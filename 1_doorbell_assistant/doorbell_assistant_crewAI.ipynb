{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Network Assistant\n",
    "---\n",
    "\n",
    "In this notebook, we will create our first crew agent. This crew agent is the Home Network Agent. This agent will have access to a knowledge base for retrieval at runtime which will contain API specs on the home networking system. In this notebook, we will create a Knowledge base for our Home Network Agent. This agent will have access to a knowledge base for retrieval at runtime which will contain API specs on the home networking system. \n",
    "\n",
    "CrewAI enables you to create AI teams where each agent has specific roles, tools, and goals, working together to accomplish complex tasks.\n",
    "\n",
    "Think of it as assembling your dream team - each member (agent) brings unique skills and expertise, collaborating seamlessly to achieve your objectives. For more information on CrewAI, view [here](https://docs.crewai.com/introduction)\n",
    "\n",
    "**Agent code generation and execution workflow**:\n",
    "\n",
    "1. The workflow starts with information retrieval. We create a knowledge base, and store the information from the `Home Network openAPI spec` into the knowledge base. The OpenAPI spec is provided by the user in the `data` folder.\n",
    "\n",
    "1. This knowledge base will be wrapped within a lambda function that will be invoked based on the user query. It will `retrieve` the top `k` results from the knowledge base and send it as an input to the next step.\n",
    "\n",
    "1. Next, a `Home Networking` assistant agent will have access to the required API specs to use to generate code. It will use the information from the Knowledge base and generate code for the given API spec, save the code and execute the code based on the parameters provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Agent\n",
    "\n",
    "On this section we declare global variables that will be act as helpers during entire notebook and you will start to create your first agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install crew ai. For installation steps, follow the instructions here: https://docs.crewai.com/installation\n",
    "!pip install 'crewai[tools]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your boto3 version\n",
    "!pip freeze | grep boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages and libraries\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import logging\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "# Get the current file's directory\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "print(parent_dir)\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "from globals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables that are defined in the \".env\" file.\n",
    "load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the sts client, the boto3 session and other variables\n",
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = \"us-east-1\" if session.region_name is None else session.region_name\n",
    "account_id_suffix = account_id[:3]\n",
    "agent_suffix = f\"{region}-{account_id_suffix}\"\n",
    "\n",
    "s3_client = boto3.client('s3', region)\n",
    "bedrock_client = boto3.client('bedrock-runtime', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "sys.path.insert(1, \"..\")\n",
    "# Import utility functions and helper functions for agents\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the config file\n",
    "--- \n",
    "\n",
    "Load the config file that contains information on the models, data directories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path to the config file\n",
    "# This config file contains data about the directory paths, the API specs that\n",
    "# are used to generate the code, and the agent foundation models that are used to generate the code.\n",
    "BASE_DIR = os.path.abspath(sys.path[1])\n",
    "CONFIG_FPATH = os.path.join(BASE_DIR, CONFIG_FNAME)\n",
    "config_data = load_config(CONFIG_FPATH)\n",
    "logger.info(f\"Loaded config from local file system: {json.dumps(config_data, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Generation Prompt: Bedrock prompt management\n",
    "---\n",
    "\n",
    "Let's create our sample code generation prompt by leveraging on Prompt Management for Amazon Bedrock. This is the prompt that is used by the sub agent to generate the code when it calls the generate_code tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt_from_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "prompt_file_path = os.path.join(\n",
    "    config_data['dir_paths']['code_gen_prompts_prefix'],\n",
    "    config_data['dir_paths']['code_gen_prompts'].get('doorbell_code_generation_prompt')\n",
    ")\n",
    "\n",
    "absolute_prompt_fpath = os.path.join(\n",
    "    parent_dir,\n",
    "    prompt_file_path\n",
    ")\n",
    "\n",
    "prompt_template_code_gen: str = read_prompt_from_file(absolute_prompt_fpath)\n",
    "print(f\"Code generation prompt that will be saved in prompt management within Bedrock: {prompt_template_code_gen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent = boto3.client(service_name = \"bedrock-agent\", region_name = region)\n",
    "response = bedrock_agent.create_prompt(\n",
    "    name = f\"prompt-for-doorbell-configuration-code-gen\",\n",
    "    description = \"Code generation prompt template that is used by the doorbell configuration agent to generate code\",\n",
    "    variants = [\n",
    "        {\n",
    "            \"name\": \"variantOne\",\n",
    "            \"templateConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"inputVariables\": [\n",
    "                        {\n",
    "                            \"name\": \"input\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"output\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"text\": prompt_template_code_gen\n",
    "                }\n",
    "            },\n",
    "            \"templateType\": \"TEXT\"\n",
    "        }\n",
    "    ],\n",
    "    defaultVariant = \"variantOne\"\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "promptId = response[\"id\"]\n",
    "promptArn = response[\"arn\"]\n",
    "promptName = response[\"name\"]\n",
    "print(f\"Prompt ID: {promptId}\\nPrompt ARN: {promptArn}\\nPrompt Name: {promptName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a draft prompt, we can create a version from it.\n",
    "response = bedrock_agent.create_prompt_version(\n",
    "    promptIdentifier = promptId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Knowledge source\n",
    "---\n",
    "\n",
    "In this section of the solution we will be creating our Crew Agent, initialize the instruction for the home networking system, and then provide it with a `knowledge source`. Knowledge in CrewAI is a powerful system that allows AI agents to access and utilize external information sources during their tasks. Think of it as giving your agents a reference library they can consult while working. \n",
    "\n",
    "We will be creating a `JSON Knowledge Source` that will store embeddings from the `JSON` API specs that we will provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_specs: Dict = config_data['dir_paths'].get('api_specs')\n",
    "logger.info(f\"API specs that are provided are as follows: {api_specs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U langchain_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = os.path.join('..', config_data['dir_paths']['data_prefix'], api_specs.get('doorbell_api_spec'))\n",
    "\n",
    "with open(source_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    file_text = f.read()\n",
    "    \n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', \n",
    "                              region_name='us-east-1')\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=config_data['model_information']['embedding_model'],\n",
    "                                       client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=file_text, metadata={})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "db = FAISS.from_documents(docs, bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_topk = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "docs_topk = retriever_topk.invoke('What is the signal strength of my porch camera?')\n",
    "print(\"\\nTop k (k=1) retrieval results:\")\n",
    "for d in docs_topk:\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tools that the CrewAI agent will have access to\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we create different tools that the CrewAI agent will have access to. This includes a tool to query from the knowledge base, generate code based on the user query and the API to call, save the code and finally execute the code to provide the final result to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import random\n",
    "import logging\n",
    "import tempfile\n",
    "import subprocess\n",
    "from crewai.tools import tool\n",
    "from typing import List, Dict, Any\n",
    "from crewai import LLM, Agent, Task, Crew\n",
    "\n",
    "# Information on the code generation model\n",
    "code_gen_data = config_data['code_generation_model_information']\n",
    "\n",
    "def get_prompt_template(prompt_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a prompt template from Bedrock prompt management.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = boto3.client(\"bedrock-agent\", region_name=os.environ.get(\"REGION\", \"us-east-1\"))\n",
    "        result = client.get_prompt(promptIdentifier=prompt_id)\n",
    "        template = result[\"variants\"][0][\"templateConfiguration\"][\"text\"][\"text\"]\n",
    "        return template\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting prompt template: {e}\")\n",
    "        raise\n",
    "\n",
    "def call_bedrock(model_id: str, messages: list, temp: float, max_tokens: int, top_p: float) -> str:\n",
    "    \"\"\"\n",
    "    Invoke Bedrock's converse API with the provided parameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = boto3.client(\"bedrock-runtime\")\n",
    "        inference_config = {\n",
    "            \"temperature\": temp,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p\n",
    "        }\n",
    "        start = time.time()\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=[{\"text\": \"You are an expert in generating Python code for home networking APIs.\"}],\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        logger.info(f\"Bedrock response in {elapsed:.2f} sec\")\n",
    "        # Extract the generated code.\n",
    "        code = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        return code\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calling Bedrock converse: {e}\")\n",
    "        raise\n",
    "\n",
    "# ─── TOOL 0: QUERY CHUNKS FROM THE IN MEMORY JSON KNOWLEDGE ───────────────────\n",
    "@tool(\"knowledge_query_tool\")\n",
    "def knowledge_query_tool(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Searches the vector store knowledge base for relevant information based on the user's query.\n",
    "    It uses a top-k retriever (with k=1) and returns a list of chunks, where each chunk is\n",
    "    a dictionary with keys 'text' (the document content) and 'metadata' (optional metadata).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a top-k retriever from the FAISS vector store (assumed to be defined as `db`)\n",
    "        retriever_topk = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "        # Invoke the retriever with the query\n",
    "        docs_topk = retriever_topk.invoke(query)\n",
    "        results = [\n",
    "            {\"text\": doc.page_content, \"metadata\": json.dumps(getattr(doc, \"metadata\", {}))}\n",
    "            for doc in docs_topk\n",
    "        ]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying knowledge source: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ─── TOOL 1: CODE GENERATION VIA BEDROCK ──────────────────────────────────────\n",
    "@tool(\"bedrock_code_generation_tool\")\n",
    "def bedrock_code_generation_tool(query: str, knowledge_chunks: List[Dict[str, Any]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generates Python code using Bedrock, based on the user's query and knowledge chunks.\n",
    "    Returns a dictionary with code content that is used by the next save_code_tool tool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt_id = promptId\n",
    "        template = get_prompt_template(prompt_id)\n",
    "        # Combine knowledge chunks into a single string\n",
    "        knowledge_content = \"\\n\".join(chunk['text'] for chunk in knowledge_chunks)\n",
    "        formatted_prompt = template.format(\n",
    "            user_query=query,\n",
    "            knowledge_content=knowledge_content,\n",
    "            auth_token=os.getenv(\"HOME_NETWORK_AUTH_TOKEN\")\n",
    "        )\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"text\": formatted_prompt}]}]\n",
    "        # Get Bedrock model and inference parameters from environment variables.\n",
    "        model = code_gen_data.get('code_generation_model')\n",
    "        temperature = code_gen_data.get('temperature', 0.1)\n",
    "        top_p = code_gen_data.get('top_p', 0.9)\n",
    "        max_tokens = code_gen_data.get('max_tokens', 4096)\n",
    "        code_output = call_bedrock(model, messages, temperature, max_tokens, top_p)\n",
    "        # Wrap the generated code in a dictionary with the expected key.\n",
    "        return {\"code_content\": str(code_output)}\n",
    "    except Exception as err:\n",
    "        logger.error(f\"bedrock_code_generation_tool error: {err}\")\n",
    "        return {\"code_content\": f\"Error generating code: {err}\"}\n",
    "\n",
    "\n",
    "\n",
    "# ─── TOOL 2: SAVE GENERATED CODE ─────────────────────────────────────────────\n",
    "@tool(\"save_code_tool\")\n",
    "def save_code_tool(code_content: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Saves the provided code content to a temporary file and returns the file path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_dir = \"generated_code\"\n",
    "        code_content: str = code_content[\"code_content\"]\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        file_path = os.path.join(base_dir, f\"generated_code_{int(time.time())}.py\")\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(code_content)\n",
    "        logger.info(f\"Code saved to {file_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in save_code_tool: {e}\")\n",
    "        return f\"Error saving code: {e}\"\n",
    "\n",
    "\n",
    "# ─── TOOL 3: EXECUTE GENERATED CODE ───────────────────────────────────────────\n",
    "@tool(\"execute_code_tool\")\n",
    "def execute_code_tool(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Executes the code saved at file_path after the save_code_tool tool is used. This tool needs\n",
    "    to be used after the save_code_tool tool.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For debugging, log the code that is about to be executed.\n",
    "        with open(file_path, \"r\") as fp:\n",
    "            code_text = fp.read()\n",
    "        logger.info(f\"Executing code:\\n{code_text}\")\n",
    "\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, file_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=int(os.environ.get(\"CODE_EXECUTION_TIMEOUT\", \"30\")),\n",
    "            env=os.environ.copy()\n",
    "        )\n",
    "        exec_result = f\"result.stdout: {result.stdout}, stderr: {result.stderr}\"\n",
    "        logger.info(f\"Execution result: {exec_result}\")\n",
    "        return exec_result\n",
    "    except subprocess.TimeoutExpired:\n",
    "        logger.error(\"Execution timed out\")\n",
    "        return \"Execution timed out\"\n",
    "    except Exception as err:\n",
    "        logger.error(f\"Error executing code: {err}\")\n",
    "        return str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the doorbell configuration networking agent and Crew\n",
    "---\n",
    "\n",
    "In this portion of the solution, we will create an agent for home networking configuration assistance, that will have access to the tools defined above. We will then add this agent to the crew with the home networking knowledge base, which contains information about the API spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_path: str = os.path.join(config_data['dir_paths']['agent_instructions_prefix'], \n",
    "                                      config_data['dir_paths']['agent_instructions'].get('doorbell_agent_instructions'))\n",
    "agent_instruction = open(os.path.join(parent_dir, instructions_path), 'r').read()\n",
    "print(agent_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "from crewai import LLM, Agent, Task, Crew, Process\n",
    "\n",
    "# This is the FM used for the home networking sub agent\n",
    "llm_instance = LLM(\n",
    "    model=config_data['model_information']['doorbell_sub_agent_model'],\n",
    "    temperature=0.1,\n",
    "    max_tokens=2048,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Create the home networking sub agent along with the tools assigned to it\n",
    "doorbell_agent = Agent(\n",
    "    role=\"Doorbell Configuration Assistant\",\n",
    "    goal=( \"always use the knowledge base tool, then generate the code, then save the code and finally and always, execute the code\" + agent_instruction\n",
    "    ),\n",
    "    backstory=(\n",
    "        \"I am an expert agent who looks for relevant API specs in the knowledge based on a user query, and then only I run the tools I have access to. I only run the knowledge base tool, generate code tool, save the code. Finally I will execute the code.\"\n",
    "    ),\n",
    "    tools=[knowledge_query_tool, bedrock_code_generation_tool, save_code_tool, execute_code_tool],\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    llm=llm_instance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tasks\n",
    "knowledge_query_task = Task(\n",
    "    description=\"Search the knowledge base for relevant information based on the user's query: '{query}'\",\n",
    "    expected_output=\"A list of knowledge chunks from the API spec relevant to the user's query.\",\n",
    "    agent=doorbell_agent,\n",
    "    output_format=\"list\"\n",
    ")\n",
    "\n",
    "code_generation_task = Task(\n",
    "    description=(\n",
    "        \"Using the user's query and the retrieved knowledge chunks, generate the appropriate Python code.\"\n",
    "    ),\n",
    "    expected_output=\"The generated Python code.\",\n",
    "    agent=doorbell_agent,\n",
    "    output_format=\"json\",\n",
    "    context=[knowledge_query_task]\n",
    ")\n",
    "\n",
    "save_code_task = Task(\n",
    "    description=\"Save the generated Python code to a file.\",\n",
    "    expected_output=\"The file path where the code is saved.\",\n",
    "    agent=doorbell_agent,\n",
    "    output_format=\"text\",\n",
    "    context=[code_generation_task]\n",
    ")\n",
    "\n",
    "execute_code_task = Task(\n",
    "    description=\"Execute the saved Python code using the file path from the save code task. Always run this task.\",\n",
    "    expected_output=\"Output text after executing the python code.\",\n",
    "    agent=doorbell_agent,\n",
    "    output_format=\"text\",\n",
    "    context=[save_code_task]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the manager agent that will be responsible to check for responses and coordinating the agentic workflow - this would usually be a \n",
    "# reasoning model\n",
    "manager_llm = LLM(\n",
    "    # model=f\"bedrock/{config_data['model_information']['home_network_sub_agent_model']}\",\n",
    "    model='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    temperature=0.1,\n",
    "    timeout=120,\n",
    "    max_tokens=256,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, create a crew that holds the agent and its task.\n",
    "crew = Crew(\n",
    "    agents=[doorbell_agent],\n",
    "    tasks=[knowledge_query_task, code_generation_task, save_code_task, execute_code_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential,\n",
    "    # manager_llm=manager_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = 'I want to get email notifications for deliveries but push notifications for when someone rings the doorbell.'\n",
    "result = crew.kickoff(inputs={\"query\": user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
